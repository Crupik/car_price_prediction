# Car Price Prediction Project

Этот проект направлен на прогнозирование цены автомобилей на основе различных признаков. В нём реализована полная цепочка обработки данных — от загрузки и предобработки до обучения модели и оценки её качества. Для решения задачи использовались методы Feature Engineering, а также модели RandomForest и XGBoost (сравнение которых помогает понять преимущества каждого подхода).


## Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [Data Preprocessing and Feature Engineering](#data-preprocessing-and-feature-engineering)
- [Models](#models)
- [Results](#results)
- [Usage](#usage)
- [Future Work](#future-work)
- [License](#license)
- [Contact](#contact)


## Overview
В данном проекте мы прогнозируем цену автомобиля на основе информации о нем (пробег, возраст, марка, модель, тип топлива, трансмиссия и т.д.).  
Основные цели:
- Очистка и предобработка данных.
- Проведение Feature Engineering (масштабирование, логарифмическое преобразование, создание новых признаков, кодирование категориальных признаков).
- Обучение модели с использованием RandomForestRegressor.
- Сравнение результатов модели RandomForest с моделью XGBoost.
- Оценка модели с помощью метрик MAE, MSE и \(R^2\).


## Dataset
Исходные данные находятся в файле `cars_raw.csv`. В датасете содержатся различные характеристики автомобилей. Некоторые столбцы (например, `SellerName`, `StreetName`, `Zipcode`, `DealType`, `VIN`, `Stock#`, `Engine`, `ExteriorColor`) были удалены, так как они не являются информативными для прогнозирования цены.


## Data Preprocessing and Feature Engineering
Основные этапы предобработки и Feature Engineering включают:
- **Очистку данных:** Замена значений `"Not Priced"` на NaN, удаление ненужных столбцов, очистка столбца `Price` (удаление знаков доллара и запятых) и преобразование в числовой формат.

- **Создание нового признака:** Вычисление возраста автомобиля (`CarAge = 2022 - Year`) и его масштабирование с помощью MinMaxScaler.

- **Логарифмическое преобразование:** Применение `np.log1p()` к признакам `Mileage` и `Price` для стабилизации распределения.

- **Очистку от выбросов:** В данных присутствовали редкие автомобили с экстремальными ценами (Lamborghini Urus и др.), что могло негативно повлиять на обучение модели. Для очистки данных были выбраны границы 0.001 и 0.9987 квантилей, чтобы сохранить дорогие автомобили, но исключить явные шумовые выбросы.

- **Преобразование категориальных признаков:**  
  - Применение One-Hot Encoding для признаков `Make` и `Drivetrain`.
  - Применение Target Encoding для признака `Model` (замена на среднее значение цены для каждой модели).
  - Использование Label Encoding для остальных категориальных признаков (например, `Used/New`, `SellerType`, `State`, `InteriorColor`, `FuelType`, `Transmission`).


## Models
В проекте протестированы две модели:
- **XGBoostRegressor:** Основная модель, показавшая хорошие результаты с параметрами `learning_rate=0.12`,`max_depth=8` и `n_estimators=270`. *Для оптимизации гиперпараметров использовался GridSearchCV.
- **RandomForestRegressor:** Модель, которая была реализована для сравнения, но в данном случае показала чуть худшие результаты.

При предобработке данных тестировались различные пороговые значения для отсеивания выбросов. Руководствуясь графиком boxplot_of_price.png, были выбраны границы 0.001 и 0.9987 квантилей. Это позволило сохранить информацию о дорогих автомобилях, но при этом исключить редкие аномальные значения, которые могли негативно повлиять на модель. 


## Results
После обратного преобразования логарифмических значений получены следующие результаты:

1. Были протестированы модели RandomForest и XGBoost
2. Для оценки результатов и сравнения моделей между собой были использованы метрики MAE, MSE, R²
3. Таблица с результатами:

| Модель        | MAE      | MSE            | R²     |
|---------------|----------|----------------|--------|
| RandomForest  | 2230.63  | 12,257,963.55  | 0.958  |
| XGBoost       | 2060.91  | 11,467,486.71  | 0.961  |

4. Почему градиентный бустинг превзошёл случайный лес?
В ходе экспериментов по прогнозированию цены автомобилей мы протестировали две модели:

RandomForestRegressor: ансамблевый метод, который обучает множество независимых деревьев и объединяет их предсказания.
XGBoost (градиентный бустинг): последовательный метод, в котором каждое новое дерево исправляет ошибки предыдущего, оптимизируя функцию потерь с использованием градиентного спуска.

Результаты мы можем видеть выше (см. пункт 3).

Возможные причины, почему XGBoost показал лучшие результаты:

Секвенциальное обучение:
Градиентный бустинг обучает деревья последовательно, что позволяет каждой новой модели исправлять ошибки предыдущих. Это может давать более точное приближение к сложным зависимостям в данных.

Оптимизация функции потерь:
XGBoost оптимизирует функцию потерь с использованием градиентного спуска, что даёт возможность лучше настраивать модель на особенности данных.

Гиперпараметрическая настройка:
С помощью GridSearchCV удалось подобрать оптимальные значения гиперпараметров для XGBoost, что положительно сказалось на его производительности.

Более эффективное использование признаков:
Благодаря последовательной корректировке ошибок модель бустинга может лучше выявлять важные признаки и их взаимодействия, даже если отдельные признаки (например, текстовые данные) имеют невысокую линейную корреляцию с целевой переменной.

Вывод:
Модель XGBoost продемонстрировала чуть лучшие показатели по всем основным метрикам (MAE, MSE, R²) по сравнению с RandomForest. Это свидетельствует о том, что градиентный бустинг более эффективно справляется с нелинейными зависимостями в данных, что особенно важно при прогнозировании цены автомобилей.

5. Кривые обучения:

XGBoost показал стабильное уменьшение ошибки на тестовых данных, а разница между обучающей и тестовой ошибкой остаётся небольшой, что свидетельствует о хорошей обобщающей способности.

RandomForest также даёт хорошую точность, но обучающая ошибка ниже, чем у XGBoost, что может говорить о небольшом переобучении.


## Usage
Чтобы запустить проект:

1. Склонируйте репозиторий:
   ```bash
   git clone https://github.com/Crupik/car_price_prediction.git
   cd car_price_prediction

2. Установите зависимости:
   ```bash
   pip install -r requirements.txt

3. Поместите файл cars_raw.csv в папку data/ (если он там не находится).

4. Запустите основные скрипты:
   ```bash
   python src/main_XGB.py
   python src/main_RF.py


## Future-work
Возможные улучшения проекта:

  Разработать API для развертывания модели с использованием Flask или FastAPI.
  Провести сравнение с дополнительными моделями, такими как SVM или нейронные сети.


## License
Этот проект лицензирован под MIT License.


## Contact
Для вопросов и предложений, пожалуйста, свяжитесь со мной по адресу [asifgamidov@mail.ru].

